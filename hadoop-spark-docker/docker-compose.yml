version: '3'
services:
  namenode:
    build: ./docker/hadoop
    hostname: namenode
    environment:
      - HADOOP_HOME=/opt/hadoop
    ports:
      - "9870:9870"
    volumes:
      - hadoop_namenode:/opt/hadoop/dfs/name
    command: sh -c "mkdir -p /opt/hadoop/logs && /opt/hadoop/bin/hdfs namenode -format && /opt/hadoop/bin/hdfs --daemon start namenode && tail -f /opt/hadoop/logs/*"

  datanode:
    build: ./docker/hadoop
    environment:
      - HADOOP_HOME=/opt/hadoop
    volumes:
      - hadoop_datanode:/opt/hadoop/dfs/data
    command: sh -c "mkdir -p /opt/hadoop/logs && /opt/hadoop/bin/hdfs --daemon start datanode && tail -f /opt/hadoop/logs/*"
    deploy:
      resources:
        limits:
          memory: 2G
    restart: unless-stopped
    depends_on:
      - namenode

  sparkmaster:
    build: ./docker/spark
    hostname: sparkmaster
    environment:
      - SPARK_HOME=/opt/spark
      - SPARK_MASTER_HOST=sparkmaster
    ports:
      - "8080:8080"
      - "7077:7077"
    command: sh -c "/opt/spark/sbin/start-master.sh && tail -f /opt/spark/logs/*"

  sparkworker:
    build: ./docker/spark
    environment:
      - SPARK_HOME=/opt/spark
      - SPARK_MASTER=spark://sparkmaster:7077
    command: sh -c "/opt/spark/sbin/start-slave.sh spark://sparkmaster:7077 && tail -f /opt/spark/logs/*"
    deploy:
      resources:
        limits:
          memory: 2G
    restart: unless-stopped
    depends_on:
      - sparkmaster

volumes:
  hadoop_namenode:
  hadoop_datanode: